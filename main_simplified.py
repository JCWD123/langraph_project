import uuid
import json
from dotenv import load_dotenv
from langchain_core.messages import HumanMessage
from simplified_graph import create_simplified_graph, stream_simplified_updates
from graph.state import plan_state
from reportlab.lib.pagesizes import A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from datetime import datetime
import pandas as pd
import os
import sys
import re
from suppliers_config import DEFAULT_SUPPLIERS
from data_processor import DataProcessor
import glob

def register_chinese_font():
    """æ³¨å†Œä¸­æ–‡å­—ä½“"""
    try:
        # å°è¯•æ³¨å†Œç³»ç»Ÿä¸­çš„ä¸­æ–‡å­—ä½“
        font_paths = [
            '/System/Library/Fonts/STHeiti Light.ttc',  # macOS
            '/System/Library/Fonts/Helvetica.ttc'     # macOSå¤‡é€‰
        ]
        
        for font_path in font_paths:
            if os.path.exists(font_path):
                pdfmetrics.registerFont(TTFont('ChineseFont', font_path))
                return True
    except:
        pass
    return False

def clean_content(content):
    """æ¸…æ´—å†…å®¹"""
    if not isinstance(content, str):
        content = str(content)
    
    # åŸºç¡€æ¸…æ´—
    content = re.sub(r'\s+', ' ', content)  # å¤šç©ºæ ¼åˆå¹¶
    content = content.strip()
    
    return content

def create_pdf_report(supplier_data, filename):
    """åˆ›å»ºPDFæŠ¥å‘Š"""
    doc = SimpleDocTemplate(filename, pagesize=A4, topMargin=1*inch, bottomMargin=1*inch)
    styles = getSampleStyleSheet()
    story = []
    
    supplier_name = supplier_data['supplier']
    response = supplier_data['response']
    
    # æ¸…æ´—å†…å®¹
    cleaned_response = clean_content(response)
    
    # æ·»åŠ æŠ¥å‘Šæ ‡é¢˜
    title_style = ParagraphStyle(
        'Title',
        parent=styles['Title'],
        fontSize=18,
        spaceAfter=24,
        spaceBefore=12,
        alignment=1,  # å±…ä¸­
    )
    
    story.append(Paragraph(f"{supplier_name} äº§å“ä¿¡æ¯æŠ¥å‘Š", title_style))
    story.append(Paragraph(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}", styles['Normal']))
    story.append(Spacer(1, 0.3*inch))
    
    # æ·»åŠ å†…å®¹
    for line in cleaned_response.split('\n'):
        if line.strip():
            story.append(Paragraph(line, styles['Normal']))
    
    # æ„å»ºPDF
    doc.build(story)
    print(f"  â†’ PDFå·²ç”Ÿæˆ: {filename}")

def save_json_data(data, filename):
    """ä¿å­˜JSONæ ¼å¼æ•°æ®"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return f"âœ… JSONæ–‡ä»¶å·²ä¿å­˜: {filename}"
    except Exception as e:
        return f"âŒ JSONä¿å­˜å¤±è´¥: {str(e)}"

def print_json_summary(supplier_data):
    """æ‰“å°ä¼ä¸šæ•°æ®çš„JSONæ‘˜è¦"""
    summary = {
        "ä¼ä¸šåç§°": supplier_data["supplier"],
        "å®˜ç½‘é“¾æ¥": supplier_data.get("structured_data", {}).get("å®˜ç½‘é“¾æ¥", "å®˜ç½‘æœªå…¬å¸ƒ"),
        "æ•°æ®å¯ä¿¡åº¦": supplier_data.get("validation", {}).get("æ•´ä½“è¯„çº§", "æœªè¯„çº§"),
        "åŸºç¡€ä¿¡æ¯å®Œæ•´åº¦": supplier_data.get("validation", {}).get("åŸºç¡€ä¿¡æ¯å®Œæ•´åº¦", "0/4"),
        "äº§å“ä¿¡æ¯å®Œæ•´åº¦": supplier_data.get("validation", {}).get("äº§å“ä¿¡æ¯å®Œæ•´åº¦", "0/4"),
        "ä¸»è¥äº§å“": supplier_data.get("structured_data", {}).get("ä¸»è¥äº§å“ç±»åˆ«", "å®˜ç½‘æœªå…¬å¸ƒ"),
        "æˆç«‹æ—¶é—´": supplier_data.get("structured_data", {}).get("æˆç«‹æ—¶é—´", "å®˜ç½‘æœªå…¬å¸ƒ")
    }
    
    print("ğŸ“Š ç»“æ„åŒ–æ•°æ®æ‘˜è¦:")
    print(json.dumps(summary, ensure_ascii=False, indent=2))
    return summary

def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨ç®€åŒ–å·¥ä½œæµï¼Œè¾“å‡ºJSONå’ŒCSVæ•°æ®"""
    load_dotenv()
    
    print("ğŸš€ ä¼ä¸šä¿¡æ¯æ”¶é›†ç³»ç»Ÿï¼ˆç®€åŒ–ç‰ˆ - JSON/CSVè¾“å‡ºï¼‰")
    print("="*60)
    print("ğŸ“‹ è¾“å‡ºæ ¼å¼ï¼š")
    print("   â€¢ JSONæ ¼å¼ï¼šæ¯ä¸ªä¼ä¸šçš„ç»“æ„åŒ–æ•°æ®")
    print("   â€¢ CSVæ ¼å¼ï¼šæ‰€æœ‰ä¼ä¸šçš„æ±‡æ€»è¡¨æ ¼") 
    print("   â€¢ æ§åˆ¶å°ï¼šå®æ—¶æ•°æ®æ‘˜è¦")
    print("   â€¢ PDFæŠ¥å‘Šï¼šå¯é€‰ç”Ÿæˆ")
    print("="*60)
    
    # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
    data_processor = DataProcessor()
    
    # è¯»å–ä¼ä¸šåˆ—è¡¨
    try:
        # æŸ¥æ‰¾æœ€æ–°çš„æœ‰å®˜ç½‘ä¼ä¸šæ–‡ä»¶
        pattern = "ä¼ä¸šåç§°_æœ‰å®˜ç½‘_*.xlsx"
        files = glob.glob(pattern)
        
        if files:
            # ä½¿ç”¨æœ€æ–°çš„æ¸…æ´—æ–‡ä»¶
            latest_file = max(files, key=os.path.getctime)
            suppliers_df = pd.read_excel(latest_file)
            suppliers = [c for c in suppliers_df['ä¼ä¸šåç§°']]
            print(f"âœ“ ä»æ¸…æ´—åæ–‡ä»¶è¯»å–ä¾›åº”å•†åˆ—è¡¨: {latest_file}")
            print(f"âœ“ å·²åŠ è½½ {len(suppliers)} ä¸ªæœ‰å®˜ç½‘çš„ä¼ä¸š")
            
            # åˆ›å»ºä¼ä¸š-å®˜ç½‘æ˜ å°„
            company_website_map = dict(zip(suppliers_df['ä¼ä¸šåç§°'], suppliers_df['ç½‘å€']))
            print(f"âœ“ åˆ›å»ºä¼ä¸šå®˜ç½‘æ˜ å°„ï¼ŒåŒ…å« {len(company_website_map)} ä¸ªå®˜ç½‘")
        else:
            # å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨åŸå§‹æ–‡ä»¶
            suppliers_df = pd.read_excel('ä¼ä¸šåç§°.xlsx')
            suppliers = [c for c in suppliers_df['ä¼ä¸šåç§°']]
            company_website_map = {}
            print("âœ“ ä»åŸå§‹Excelæ–‡ä»¶è¯»å–ä¾›åº”å•†åˆ—è¡¨")
    except:
        suppliers = DEFAULT_SUPPLIERS
        company_website_map = {}
        print("âœ“ ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤ä¾›åº”å•†åˆ—è¡¨")
    
    # å‘½ä»¤è¡Œå‚æ•°æ”¯æŒ
    if len(sys.argv) > 1:
        suppliers = sys.argv[1:]
        print(f"âœ“ ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„ä¾›åº”å•†: {suppliers}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"ä¼ä¸šæ•°æ®_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    print(f"âœ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆ›å»ºç®€åŒ–çš„çŠ¶æ€å›¾
    config = {
        "configurable": {"thread_id": uuid.uuid4().hex},
        "recursion_limit": 50
    }
    graph = create_simplified_graph()
    
    all_suppliers_data = []
    structured_data_list = []
    json_summary_list = []
    
    print("\nå¼€å§‹ä¼ä¸šä¿¡æ¯æ”¶é›†...")
    print(f"è®¡åˆ’æŸ¥è¯¢ {len(suppliers)} ä¸ªä¼ä¸š")
    print("=" * 50)
    
    # å¤„ç†æ¯ä¸ªä¼ä¸š
    for i, supplier in enumerate(suppliers, 1):
        print(f"\nğŸ“‹ ç¬¬ {i}/{len(suppliers)} ä¸ªä¼ä¸š: {supplier}")
        print("-" * 40)
        
        # ä½¿ç”¨ç®€åŒ–çš„æŸ¥è¯¢
        simple_query = f"è¯·æ”¶é›† {supplier} çš„è¯¦ç»†äº§å“ä¿¡æ¯å’Œä¼ä¸šä¿¡æ¯"
        print(f"ğŸ“ æŸ¥è¯¢: {simple_query}")
        
        # åˆ›å»ºçŠ¶æ€
        state = plan_state(
            task=simple_query,
            goal=f'æ”¶é›†{supplier}çš„ä¼ä¸šä¿¡æ¯',
            messages=[HumanMessage(content=simple_query)],
            steps=[],
            steps2results={},
            documents=[],
            current_step=0,
            company_website=company_website_map.get(supplier, "")
        )
        
        ai_response = ""
        
        print("ğŸ” å¼€å§‹æœç´¢...")
        try:
            for answer in stream_simplified_updates(graph, state, config):
                print(answer, end="")
                ai_response += str(answer)
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢ {supplier} æ—¶å‡ºé”™: {e}")
            ai_response = f"æŸ¥è¯¢å¤±è´¥: {str(e)}"
        
        print()  # æ¢è¡Œ
        
        # æå–ç»“æ„åŒ–æ•°æ®
        print("ğŸ“Š æå–ç»“æ„åŒ–æ•°æ®...")
        structured_data = data_processor.extract_structured_data(supplier, ai_response)
        structured_data_list.append(structured_data)
        
        # éªŒè¯æ•°æ®è´¨é‡
        validation_result = data_processor.validate_official_data(structured_data)
        print(f"   ğŸ“ˆ æ•°æ®è´¨é‡: {validation_result.get('æ•´ä½“è¯„çº§', 'Cçº§')}")
        print(f"   ğŸ”— å®˜ç½‘é“¾æ¥: {validation_result.get('å®˜ç½‘é“¾æ¥', 'âŒ')}")
        
        # ä¿å­˜æ•°æ®
        supplier_data = {
            "supplier": supplier,
            "query": simple_query,
            "response": ai_response,
            "structured_data": structured_data,
            "validation": validation_result,
            "timestamp": datetime.now().isoformat(),
            "known_website": company_website_map.get(supplier, "")
        }
        all_suppliers_data.append(supplier_data)
        
        # æ‰“å°JSONæ‘˜è¦å¹¶ä¿å­˜
        json_summary = print_json_summary(supplier_data)
        json_summary_list.append(json_summary)
        
        # ä¿å­˜å•ä¸ªä¼ä¸šçš„JSONæ–‡ä»¶
        individual_json_file = os.path.join(output_dir, f"{supplier}_{timestamp}.json")
        json_result = save_json_data(supplier_data, individual_json_file)
        print(f"   {json_result}")
        
        print(f"âœ… {supplier} å¤„ç†å®Œæˆ")
    
    print("\n" + "=" * 50)
    print("ğŸ‰ æ‰€æœ‰ä¼ä¸šå¤„ç†å®Œæˆ!")
    
    # å¯¼å‡ºæ±‡æ€»æ•°æ®
    print("\nğŸ“Š å¯¼å‡ºæ±‡æ€»æ•°æ®...")
    
    # 1. ä¿å­˜æ‰€æœ‰ä¼ä¸šçš„JSONæ±‡æ€»
    all_data_json_file = os.path.join(output_dir, f"æ‰€æœ‰ä¼ä¸šæ•°æ®_{timestamp}.json")
    json_result = save_json_data(all_suppliers_data, all_data_json_file)
    print(f"   {json_result}")
    
    # 2. ä¿å­˜JSONæ‘˜è¦ï¼ˆä¾¿äºå¿«é€ŸæŸ¥çœ‹ï¼‰
    summary_json_file = os.path.join(output_dir, f"ä¼ä¸šæ•°æ®æ‘˜è¦_{timestamp}.json")
    summary_result = save_json_data(json_summary_list, summary_json_file)
    print(f"   {summary_result}")
    
    # 3. CSVå¯¼å‡º
    if structured_data_list:
        csv_filename = os.path.join(output_dir, f"ä¼ä¸šæ•°æ®_{timestamp}.csv")
        csv_result = data_processor.export_to_csv(structured_data_list, csv_filename)
        print(f"   {csv_result}")
        
        # 4. Excelå¯¼å‡ºï¼ˆåŒ…å«æ•°æ®è´¨é‡ç»Ÿè®¡ï¼‰
        excel_filename = os.path.join(output_dir, f"ä¼ä¸šæ•°æ®_{timestamp}.xlsx")
        excel_result = data_processor.export_to_excel(structured_data_list, excel_filename)
        print(f"   {excel_result}")
        
        # 5. æ•°æ®è´¨é‡ç»Ÿè®¡
        print("\nğŸ“ˆ æ•°æ®è´¨é‡ç»Ÿè®¡:")
        quality_stats = data_processor.generate_quality_statistics(structured_data_list)
        
        print("   å‰5ä¸ªå­—æ®µå®Œæ•´åº¦:")
        for i, stat in enumerate(quality_stats[:5], 1):
            print(f"   {i}. {stat['å­—æ®µåç§°']:15s} - {stat['å®Œæ•´åº¦ç™¾åˆ†æ¯”']:>6s}")
        
        # 6. å¯ä¿¡åº¦åˆ†å¸ƒ
        credibility_distribution = {}
        for data in structured_data_list:
            credibility = data.get("æ•°æ®å¯ä¿¡åº¦", "Dçº§ - æ•°æ®ä¸è¶³")
            credibility_distribution[credibility] = credibility_distribution.get(credibility, 0) + 1
        
        print(f"\n   æ•°æ®å¯ä¿¡åº¦åˆ†å¸ƒ:")
        for credibility, count in credibility_distribution.items():
            percentage = (count / len(structured_data_list)) * 100
            print(f"      {credibility}: {count}å®¶ ({percentage:.1f}%)")
    
    # ç”Ÿæˆæ•°æ®è®¿é—®æŒ‡å—
    print(f"\nğŸ“ æ•°æ®è¾“å‡ºæ€»ç»“:")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    print(f"   ğŸ“Š æ•°æ®æ ¼å¼:")
    print(f"      â€¢ JSONæ±‡æ€»: æ‰€æœ‰ä¼ä¸šæ•°æ®_{timestamp}.json")
    print(f"      â€¢ JSONæ‘˜è¦: ä¼ä¸šæ•°æ®æ‘˜è¦_{timestamp}.json")
    print(f"      â€¢ CSVè¡¨æ ¼: ä¼ä¸šæ•°æ®_{timestamp}.csv")
    print(f"      â€¢ ExcelæŠ¥å‘Š: ä¼ä¸šæ•°æ®_{timestamp}.xlsx")
    print(f"      â€¢ å•ä¼ä¸šJSON: [ä¼ä¸šåç§°]_{timestamp}.json")
    
    # è¿”å›ç»“æ„åŒ–æ•°æ®ï¼ˆç”¨äºAPIè°ƒç”¨ç­‰ï¼‰
    return {
        "summary": {
            "total_companies": len(suppliers),
            "successful_queries": len([d for d in all_suppliers_data if "æŸ¥è¯¢å¤±è´¥" not in d["response"]]),
            "output_directory": output_dir,
            "timestamp": timestamp
        },
        "data": all_suppliers_data,
        "json_summaries": json_summary_list,
        "structured_data": structured_data_list
    }

if __name__ == "__main__":
    result = main()
    
    # å¦‚æœä½œä¸ºæ¨¡å—è°ƒç”¨ï¼Œå¯ä»¥è¿”å›æ•°æ®
    print(f"\nğŸ’¡ ç¨‹åºå®Œæˆï¼Œè¿”å›äº†åŒ…å« {len(result['data'])} ä¸ªä¼ä¸šçš„ç»“æ„åŒ–æ•°æ®")
    print(f"ğŸ“‹ æ•°æ®å¯é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¿é—®:")
    print(f"   - result['data']: å®Œæ•´ä¼ä¸šæ•°æ®")
    print(f"   - result['json_summaries']: ä¼ä¸šæ‘˜è¦æ•°æ®")
    print(f"   - result['structured_data']: CSVç»“æ„åŒ–æ•°æ®") 