from langgraph.graph.state import CompiledStateGraph
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, END
from langgraph.graph.state import StateGraph
from langchain_core.messages import HumanMessage, AIMessage

from paper_agent.web_search import web_search
from graph.state import plan_state
from model.llm import get_model
import re
import pandas as pd

def extract_domain(url):
    """ä»URLä¸­æå–åŸŸå"""
    if pd.isna(url) or url == '-':
        return 'æ— åŸŸå'
    
    try:
        url = str(url).strip()
        if url.startswith('http'):
            # æå–åŸŸåéƒ¨åˆ†
            domain = url.split('/')[2]
        else:
            domain = url.split('/')[0]
        
        # ç®€åŒ–åŸŸåï¼ˆç§»é™¤wwwç­‰å‰ç¼€ï¼‰
        if domain.startswith('www.'):
            domain = domain[4:]
        
        return domain
    except:
        return 'è§£æå¤±è´¥'

def direct_search_agent(state: plan_state):
    """
    ä¸¤é˜¶æ®µæœç´¢éªŒè¯ä»£ç†ï¼š
    1. åˆæ­¥æœç´¢ä¼ä¸šä¿¡æ¯ï¼ˆä¼˜å…ˆå®˜ç½‘ï¼‰
    2. å¤§æ¨¡å‹åˆ†æç”Ÿæˆæ‰©å±•æŸ¥è¯¢
    3. éªŒè¯æœç´¢ï¼ˆé‡ç‚¹æŸ¥å®˜ç½‘ï¼‰
    4. å‰”é™¤å®˜ç½‘ä¸­æœªå‡ºç°çš„ä¿¡æ¯
    """
    task = state.get("task", "")
    company_website = state.get("company_website", "")
    
    # ä»ä»»åŠ¡ä¸­æå–ä¼ä¸šåç§°
    company_name = extract_company_name(task)
    if not company_name:
        error_msg = "æ— æ³•ä»æŸ¥è¯¢ä¸­æå–åˆ°å…·ä½“çš„ä¼ä¸šåç§°"
        state["messages"].append(AIMessage(content=error_msg))
        return state
    
    print(f"ğŸ¢ ç›®æ ‡ä¼ä¸š: {company_name}")
    if company_website:
        print(f"ğŸŒ å·²çŸ¥å®˜ç½‘: {company_website}")
    
    # ç¬¬ä¸€é˜¶æ®µï¼šåˆæ­¥æœç´¢ï¼ˆä¼˜å…ˆå®˜ç½‘ï¼‰
    print(f"\nğŸ“ ç¬¬ä¸€é˜¶æ®µï¼šåˆæ­¥æœç´¢ä¼ä¸šä¿¡æ¯")
    initial_results = perform_initial_search(company_name, company_website)
    
    if not initial_results:
        error_msg = f"æœªèƒ½æ‰¾åˆ°å…³äº {company_name} çš„åˆæ­¥ä¿¡æ¯"
        state["messages"].append(AIMessage(content=error_msg))
        return state
    
    # ç¬¬äºŒé˜¶æ®µï¼šå¤§æ¨¡å‹åˆ†æå’Œç”Ÿæˆæ‰©å±•æŸ¥è¯¢
    print(f"\nğŸ¤– ç¬¬äºŒé˜¶æ®µï¼šå¤§æ¨¡å‹åˆ†æåˆæ­¥ç»“æœ")
    analysis_result = analyze_and_generate_queries(company_name, initial_results, company_website)
    
    # ç¬¬ä¸‰é˜¶æ®µï¼šéªŒè¯æœç´¢ï¼ˆé‡ç‚¹æŸ¥å®˜ç½‘ï¼‰
    print(f"\nğŸ” ç¬¬ä¸‰é˜¶æ®µï¼šéªŒè¯æœç´¢ï¼ˆå®˜ç½‘ä¼˜å…ˆï¼‰")
    verification_results = perform_verification_search(company_name, analysis_result["verification_queries"])
    
    # ç¬¬å››é˜¶æ®µï¼šä¿¡æ¯éªŒè¯å’Œå‰”é™¤
    print(f"\nâœ… ç¬¬å››é˜¶æ®µï¼šä¿¡æ¯éªŒè¯å’Œå‰”é™¤")
    final_report = verify_and_filter_information(
        company_name, 
        analysis_result["extracted_info"], 
        verification_results,
        company_website
    )
    
    # ä¿å­˜ç»“æœ
    state["messages"].append(AIMessage(content=final_report))
    state["documents"].extend(initial_results + verification_results)
    
    return state

def perform_initial_search(company_name: str, company_website: str = "") -> list:
    """ç¬¬ä¸€é˜¶æ®µï¼šåˆæ­¥æœç´¢ä¼ä¸šåŸºç¡€ä¿¡æ¯ï¼ˆä¼˜å…ˆå®˜ç½‘ï¼‰"""
    initial_keywords = []
    
    # å¦‚æœæœ‰å®˜ç½‘ï¼Œä¼˜å…ˆæœç´¢å®˜ç½‘å†…å®¹
    if company_website:
        domain = extract_domain(company_website)
        initial_keywords.extend([
            f"{company_name} site:{domain}",
            f"äº§å“ site:{domain}",
            f"å…³äºæˆ‘ä»¬ site:{domain}",
            f"{company_name} å®˜ç½‘"
        ])
    
    # æ·»åŠ é€šç”¨æœç´¢å…³é”®è¯
    initial_keywords.extend([
        f"{company_name} ä¼ä¸šä¿¡æ¯", 
        f"{company_name} äº§å“ä»‹ç»",
        f"{company_name} å…¬å¸ç®€ä»‹"
    ])
    
    all_results = []
    for i, keyword in enumerate(initial_keywords, 1):
        print(f"   ğŸ” åˆæ­¥æœç´¢ {i}/{len(initial_keywords)}: {keyword}")
        
        try:
            search_result = web_search.invoke({"key_words": keyword})
            if search_result:
                print(f"      âœ… æ‰¾åˆ° {len(search_result)} æ¡ç»“æœ")
                all_results.extend(search_result)
            else:
                print(f"      âŒ æœªæ‰¾åˆ°ç»“æœ")
        except Exception as e:
            print(f"      âš ï¸ æœç´¢å‡ºé”™: {e}")
    
    print(f"   ğŸ“Š åˆæ­¥æœç´¢å®Œæˆï¼Œå…±æ”¶é›† {len(all_results)} æ¡ä¿¡æ¯")
    return all_results

def analyze_and_generate_queries(company_name: str, search_results: list, company_website: str = "") -> dict:
    """ç¬¬äºŒé˜¶æ®µï¼šå¤§æ¨¡å‹åˆ†æç»“æœå¹¶ç”Ÿæˆæ‰©å±•éªŒè¯æŸ¥è¯¢"""
    
    # åˆå¹¶æœç´¢ç»“æœå†…å®¹
    combined_content = ""
    official_urls = []
    
    for doc in search_results:
        if hasattr(doc, 'page_content'):
            combined_content += doc.page_content + "\n\n"
        if hasattr(doc, 'metadata') and doc.metadata.get('source'):
            url = doc.metadata['source']
            # è¯†åˆ«å¯èƒ½çš„å®˜ç½‘é“¾æ¥
            if any(domain in url.lower() for domain in ['.com', '.com.cn', '.cn']) and \
               not any(third_party in url.lower() for third_party in ['baidu', 'zhihu', 'csdn', 'sohu']):
                official_urls.append(url)
    
    # ä½¿ç”¨å¤§æ¨¡å‹åˆ†æ
    model = get_model()
    
    # æ„å»ºå·²çŸ¥å®˜ç½‘ä¿¡æ¯
    known_website_info = ""
    if company_website:
        known_website_info = f"\n# å·²çŸ¥å®˜ç½‘ï¼š\n{company_website}\n"
    
    analysis_prompt = f"""
åŸºäºä»¥ä¸‹å…³äº {company_name} çš„æœç´¢ç»“æœï¼Œè¯·è¿›è¡Œåˆ†æå¹¶ç”ŸæˆéªŒè¯æŸ¥è¯¢ï¼š

# æœç´¢ç»“æœå†…å®¹ï¼š
{combined_content[:2000]}...
{known_website_info}
# å¯èƒ½çš„å®˜ç½‘é“¾æ¥ï¼š
{chr(10).join(official_urls[:3])}

è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

## 1. æå–å…³é”®ä¿¡æ¯
ä»æœç´¢ç»“æœä¸­æå–ä»¥ä¸‹ä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ï¼š
- ä¸»è¥äº§å“/æœåŠ¡
- æ ¸å¿ƒæŠ€æœ¯
- ä¼ä¸šè§„æ¨¡
- æˆç«‹æ—¶é—´
- æ€»éƒ¨åœ°å€
- è®¤è¯èµ„è´¨

## 2. ç”ŸæˆéªŒè¯æŸ¥è¯¢
ä¸ºæ¯ä¸ªæå–çš„ä¿¡æ¯ç”Ÿæˆå®˜ç½‘éªŒè¯æŸ¥è¯¢å…³é”®è¯ï¼Œæ ¼å¼ï¼š
- "{company_name} [å…·ä½“äº§å“/æŠ€æœ¯] site:å®˜ç½‘åŸŸå"
- "{company_name} [å…·ä½“ä¿¡æ¯] å®˜ç½‘"

## 3. å®˜ç½‘åŸŸåè¯†åˆ«
{f"å·²çŸ¥å®˜ç½‘åŸŸå: {extract_domain(company_website)}" if company_website else "å¦‚æœèƒ½è¯†åˆ«å‡ºå®˜ç½‘åŸŸåï¼Œè¯·åˆ—å‡ºã€‚"}

è¯·ç”¨ä»¥ä¸‹JSONæ ¼å¼å›å¤ï¼š
{{
    "extracted_info": {{
        "ä¸»è¥äº§å“": "å…·ä½“äº§å“åç§°",
        "æ ¸å¿ƒæŠ€æœ¯": "å…·ä½“æŠ€æœ¯",
        "ä¼ä¸šè§„æ¨¡": "è§„æ¨¡ä¿¡æ¯",
        "æˆç«‹æ—¶é—´": "æ—¶é—´",
        "æ€»éƒ¨åœ°å€": "åœ°å€",
        "è®¤è¯èµ„è´¨": "è®¤è¯ä¿¡æ¯"
    }},
    "official_domains": ["åŸŸå1", "åŸŸå2"],
    "verification_queries": [
        "{company_name} å…·ä½“äº§å“ å®˜ç½‘",
        "{company_name} æŠ€æœ¯ site:åŸŸå"
    ]
}}
"""
    
    try:
        print(f"   ğŸ¤– å¤§æ¨¡å‹åˆ†æä¸­...")
        response = model.invoke([HumanMessage(content=analysis_prompt)])
        
        # å°è¯•è§£æJSONå“åº”
        import json
        try:
            # æå–JSONéƒ¨åˆ†
            content = response.content
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                json_str = content[json_start:json_end]
                result = json.loads(json_str)
                print(f"   âœ… åˆ†æå®Œæˆï¼Œæå–äº† {len(result.get('extracted_info', {}))} é¡¹ä¿¡æ¯")
                print(f"   ğŸ“‹ ç”Ÿæˆäº† {len(result.get('verification_queries', []))} ä¸ªéªŒè¯æŸ¥è¯¢")
                return result
        except json.JSONDecodeError:
            print(f"   âš ï¸ JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ")
            
        # å¤‡é€‰æ–¹æ¡ˆï¼šç®€å•çš„å…³é”®è¯ç”Ÿæˆ
        return generate_fallback_queries(company_name, combined_content)
        
    except Exception as e:
        print(f"   âš ï¸ å¤§æ¨¡å‹åˆ†æå‡ºé”™: {e}")
        return generate_fallback_queries(company_name, combined_content)

def generate_fallback_queries(company_name: str, content: str) -> dict:
    """å¤‡é€‰æ–¹æ¡ˆï¼šç®€å•ç”ŸæˆéªŒè¯æŸ¥è¯¢"""
    return {
        "extracted_info": {
            "ä¸»è¥äº§å“": "å¾…éªŒè¯",
            "æ ¸å¿ƒæŠ€æœ¯": "å¾…éªŒè¯", 
            "ä¼ä¸šè§„æ¨¡": "å¾…éªŒè¯"
        },
        "official_domains": [],
        "verification_queries": [
            f"{company_name} ä¸»è¥äº§å“ å®˜ç½‘",
            f"{company_name} æ ¸å¿ƒæŠ€æœ¯ å®˜ç½‘",
            f"{company_name} ä¼ä¸šä»‹ç» å®˜ç½‘",
            f"{company_name} äº§å“ä¸­å¿ƒ å®˜ç½‘"
        ]
    }

def perform_verification_search(company_name: str, verification_queries: list) -> list:
    """ç¬¬ä¸‰é˜¶æ®µï¼šæ‰§è¡ŒéªŒè¯æœç´¢"""
    all_results = []
    
    for i, query in enumerate(verification_queries, 1):
        print(f"   ğŸ” éªŒè¯æœç´¢ {i}/{len(verification_queries)}: {query}")
        
        try:
            search_result = web_search.invoke({"key_words": query})
            if search_result:
                print(f"      âœ… æ‰¾åˆ° {len(search_result)} æ¡éªŒè¯ç»“æœ")
                all_results.extend(search_result)
            else:
                print(f"      âŒ éªŒè¯æœªæ‰¾åˆ°ç»“æœ")
        except Exception as e:
            print(f"      âš ï¸ éªŒè¯æœç´¢å‡ºé”™: {e}")
    
    print(f"   ğŸ“Š éªŒè¯æœç´¢å®Œæˆï¼Œå…±æ”¶é›† {len(all_results)} æ¡éªŒè¯ä¿¡æ¯")
    return all_results

def verify_and_filter_information(company_name: str, extracted_info: dict, verification_results: list, company_website: str) -> str:
    """ç¬¬å››é˜¶æ®µï¼šéªŒè¯ä¿¡æ¯å¹¶å‰”é™¤éå®˜ç½‘å†…å®¹"""
    
    # åˆ†ç¦»å®˜ç½‘å’Œéå®˜ç½‘å†…å®¹
    official_content = []
    non_official_content = []
    official_urls = []
    
    # å¦‚æœæœ‰å·²çŸ¥å®˜ç½‘ï¼Œä¼˜å…ˆåŒ¹é…å·²çŸ¥å®˜ç½‘çš„å†…å®¹
    known_domain = extract_domain(company_website) if company_website else ""
    
    for doc in verification_results:
        if hasattr(doc, 'metadata') and doc.metadata.get('source'):
            url = doc.metadata['source']
            # åˆ¤æ–­æ˜¯å¦ä¸ºå®˜ç½‘ï¼ˆä¼˜å…ˆå·²çŸ¥å®˜ç½‘ï¼‰
            if is_official_website(url, known_domain):
                official_content.append(doc.page_content if hasattr(doc, 'page_content') else str(doc))
                official_urls.append(url)
            else:
                non_official_content.append(doc.page_content if hasattr(doc, 'page_content') else str(doc))
    
    print(f"   ğŸ“Š å®˜ç½‘å†…å®¹: {len(official_content)} æ¡")
    print(f"   ğŸ“Š éå®˜ç½‘å†…å®¹: {len(non_official_content)} æ¡")
    if known_domain:
        print(f"   ğŸ¯ å·²çŸ¥å®˜ç½‘åŸŸå: {known_domain}")
    
    # éªŒè¯æå–çš„ä¿¡æ¯
    verified_info = {}
    removed_info = {}
    
    combined_official_content = "\n".join(official_content)
    
    for key, value in extracted_info.items():
        if value and value != "å¾…éªŒè¯":
            # æ£€æŸ¥å®˜ç½‘å†…å®¹ä¸­æ˜¯å¦åŒ…å«è¯¥ä¿¡æ¯
            if verify_info_in_official_content(value, combined_official_content):
                verified_info[key] = value
                print(f"   âœ… {key}: {value} - å®˜ç½‘éªŒè¯é€šè¿‡")
            else:
                removed_info[key] = value
                print(f"   âŒ {key}: {value} - å®˜ç½‘æœªæ‰¾åˆ°ï¼Œå·²å‰”é™¤")
        else:
            # å°è¯•ä»å®˜ç½‘å†…å®¹ä¸­æå–
            extracted_value = extract_info_from_official_content(key, combined_official_content)
            if extracted_value:
                verified_info[key] = extracted_value
                print(f"   âœ… {key}: {extracted_value} - ä»å®˜ç½‘æ–°æå–")
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    report = generate_verified_report(company_name, verified_info, removed_info, official_urls, company_website)
    
    return report

def is_official_website(url: str, known_domain: str = "") -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºå®˜ç½‘ï¼ˆä¼˜å…ˆå·²çŸ¥åŸŸåï¼‰"""
    url_lower = url.lower()
    
    # å¦‚æœæœ‰å·²çŸ¥åŸŸåï¼Œä¼˜å…ˆåŒ¹é…
    if known_domain and known_domain.lower() in url_lower:
        return True
    
    # æ’é™¤æ˜æ˜¾çš„ç¬¬ä¸‰æ–¹ç½‘ç«™
    third_party_sites = [
        'baidu.com', 'google.com', 'bing.com', 'zhihu.com', 'csdn.net',
        'weibo.com', 'qq.com', 'sina.com', 'sohu.com', '163.com',
        'chinaz.com', 'tianyancha.com', 'qichacha.com', 'aiqicha.com'
    ]
    
    if any(site in url_lower for site in third_party_sites):
        return False
    
    # åŒ…å«ä¼ä¸šåŸŸåç‰¹å¾
    official_indicators = ['.com', '.com.cn', '.cn', '.net', '.org']
    return any(indicator in url_lower for indicator in official_indicators)

def verify_info_in_official_content(info: str, official_content: str) -> bool:
    """éªŒè¯ä¿¡æ¯æ˜¯å¦åœ¨å®˜ç½‘å†…å®¹ä¸­å‡ºç°"""
    if not info or not official_content:
        return False
    
    # æå–å…³é”®è¯è¿›è¡ŒåŒ¹é…
    keywords = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+', info)
    official_content_lower = official_content.lower()
    
    # è‡³å°‘æœ‰50%çš„å…³é”®è¯åœ¨å®˜ç½‘å†…å®¹ä¸­å‡ºç°
    matched_keywords = 0
    for keyword in keywords:
        if len(keyword) > 1 and keyword.lower() in official_content_lower:
            matched_keywords += 1
    
    return len(keywords) > 0 and (matched_keywords / len(keywords)) >= 0.5

def extract_info_from_official_content(info_type: str, official_content: str) -> str:
    """ä»å®˜ç½‘å†…å®¹ä¸­æå–ç‰¹å®šç±»å‹çš„ä¿¡æ¯"""
    if not official_content:
        return ""
    
    # å®šä¹‰æå–æ¨¡å¼
    patterns = {
        "ä¸»è¥äº§å“": [
            r"ä¸»è¥äº§å“[ï¼š:]\s*([^ã€‚\n]+)",
            r"ä¸»è¦äº§å“[ï¼š:]\s*([^ã€‚\n]+)",
            r"äº§å“[ï¼š:]\s*([^ã€‚\n]+)",
            r"ä¸“ä¸šç”Ÿäº§\s*([^ã€‚\n]+)"
        ],
        "æ ¸å¿ƒæŠ€æœ¯": [
            r"æ ¸å¿ƒæŠ€æœ¯[ï¼š:]\s*([^ã€‚\n]+)",
            r"æŠ€æœ¯ä¼˜åŠ¿[ï¼š:]\s*([^ã€‚\n]+)",
            r"æŠ€æœ¯ç‰¹ç‚¹[ï¼š:]\s*([^ã€‚\n]+)"
        ],
        "ä¼ä¸šè§„æ¨¡": [
            r"å‘˜å·¥[ï¼š:]\s*([^ã€‚\n]*äºº)",
            r"è§„æ¨¡[ï¼š:]\s*([^ã€‚\n]+)"
        ]
    }
    
    if info_type in patterns:
        for pattern in patterns[info_type]:
            matches = re.findall(pattern, official_content)
            if matches:
                return matches[0].strip()
    
    return ""

def generate_verified_report(company_name: str, verified_info: dict, removed_info: dict, official_urls: list, company_website: str) -> str:
    """ç”ŸæˆéªŒè¯åçš„æŠ¥å‘Š"""
    
    website_note = f"\nğŸ¢ å·²çŸ¥å®˜ç½‘ï¼š{company_website}" if company_website else ""
    
    report = f"""# {company_name} ä¼ä¸šä¿¡æ¯æŠ¥å‘Šï¼ˆå®˜ç½‘éªŒè¯ç‰ˆï¼‰

## æ•°æ®éªŒè¯è¯´æ˜
âœ… æœ¬æŠ¥å‘Šä¿¡æ¯å‡ç»è¿‡å®˜ç½‘éªŒè¯
âŒ å·²å‰”é™¤æ— æ³•åœ¨å®˜ç½‘ä¸­ç¡®è®¤çš„ä¿¡æ¯
ğŸ” æ•°æ®æ¥æºï¼šä¼˜å…ˆé‡‡ç”¨å®˜æ–¹ç½‘ç«™ä¿¡æ¯{website_note}

## ä¼ä¸šåŸºç¡€ä¿¡æ¯ï¼ˆå®˜ç½‘éªŒè¯ï¼‰

"""
    
    # æ·»åŠ éªŒè¯é€šè¿‡çš„ä¿¡æ¯
    if verified_info:
        for key, value in verified_info.items():
            report += f"**{key}**: {value}\n\n"
    else:
        report += "æš‚æ— å®Œæ•´çš„å®˜ç½‘éªŒè¯ä¿¡æ¯\n\n"
    
    # æ·»åŠ è¢«å‰”é™¤çš„ä¿¡æ¯è¯´æ˜
    if removed_info:
        report += "## å·²å‰”é™¤ä¿¡æ¯ï¼ˆå®˜ç½‘æœªç¡®è®¤ï¼‰\n\n"
        for key, value in removed_info.items():
            report += f"- {key}: {value} ï¼ˆåœ¨å®˜ç½‘ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼‰\n"
        report += "\n"
    
    # æ·»åŠ å®˜ç½‘æ¥æº
    if official_urls or company_website:
        report += "## å®˜æ–¹ä¿¡æ¯æ¥æº\n\n"
        idx = 1
        
        # å¦‚æœæœ‰å·²çŸ¥å®˜ç½‘ï¼Œä¼˜å…ˆæ˜¾ç¤º
        if company_website:
            report += f"{idx}. {company_website} (å·²çŸ¥å®˜ç½‘)\n"
            idx += 1
            
        # æ·»åŠ æœç´¢åˆ°çš„å®˜ç½‘
        for url in set(official_urls):
            if url != company_website:  # é¿å…é‡å¤æ˜¾ç¤º
                report += f"{idx}. {url}\n"
                idx += 1
    
    report += f"\n## æŠ¥å‘Šç‰¹ç‚¹\n\n"
    report += f"- âœ… ä¿¡æ¯çœŸå®æ€§ï¼šä»…åŒ…å«å®˜ç½‘ç¡®è®¤çš„ä¿¡æ¯\n"
    report += f"- âŒ ä¿¡æ¯å‰”é™¤ï¼šç§»é™¤äº† {len(removed_info)} é¡¹æ— æ³•éªŒè¯çš„ä¿¡æ¯\n"
    report += f"- ğŸ” éªŒè¯æ¥æºï¼š{len(set(official_urls))} ä¸ªå®˜æ–¹ç½‘ç«™\n"
    if company_website:
        known_domain = extract_domain(company_website)
        report += f"- ğŸ¯ å·²çŸ¥å®˜ç½‘ä¼˜å…ˆï¼šä¼˜å…ˆä»å·²çŸ¥å®˜ç½‘ {known_domain} è·å–ä¿¡æ¯\n"
    
    return report

def extract_company_name(task: str) -> str:
    """
    ä»ä»»åŠ¡æè¿°ä¸­æå–ä¼ä¸šåç§°
    """
    # å°è¯•å¤šç§æ¨¡å¼æ¥æå–ä¼ä¸šåç§°
    patterns = [
        r'è¯·ä¸ºæˆ‘æ”¶é›†\s*([^çš„]*?)\s*çš„è¯¦ç»†äº§å“ä¿¡æ¯',
        r'æ”¶é›†\s*([^çš„]*?)\s*çš„.*?ä¿¡æ¯',
        r'æŸ¥è¯¢\s*([^çš„]*?)\s*çš„.*?ä¿¡æ¯',
        r'([^ï¼Œã€‚\n]*(?:æœ‰é™å…¬å¸|è‚¡ä»½æœ‰é™å…¬å¸|é›†å›¢|ç§‘æŠ€|å®ä¸š))',
        r'([^ï¼Œã€‚\n]*å…¬å¸)',
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, task)
        if matches:
            company_name = matches[0].strip()
            if len(company_name) > 2:  # è¿‡æ»¤è¿‡çŸ­çš„åç§°
                return company_name
    
    return ""

def create_simplified_graph() -> CompiledStateGraph:
    """
    åˆ›å»ºç®€åŒ–çš„å·¥ä½œæµ - ç§»é™¤solve_agentï¼Œç›´æ¥è¿›è¡Œæœç´¢
    """
    workflow = StateGraph(plan_state)
    
    # åªæ·»åŠ å¿…è¦çš„èŠ‚ç‚¹
    workflow.add_node("direct_search", direct_search_agent)
    
    # ç®€å•çš„çº¿æ€§æµç¨‹
    workflow.add_edge(START, "direct_search")
    workflow.add_edge("direct_search", END)
    
    return workflow.compile(checkpointer=MemorySaver())

def stream_simplified_updates(graph: CompiledStateGraph, state: plan_state, config: dict):
    """
    æµå¼å¤„ç†ç®€åŒ–å·¥ä½œæµçš„æ›´æ–°
    """
    for chunk, _ in graph.stream(state, config, stream_mode="messages"):
        if hasattr(chunk, 'content') and chunk.content:
            yield chunk.content 